{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class TransformerModel(nn.Module):\n    def __init__(self, ntoken, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        '''\n        ntokens: the size of vocabulary\n        nhid: the hidden dimension of the model.\n        We assume that embedding_dim = nhid\n        nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n        nhead: the number of heads in the multiheadattention models\n        dropout: the dropout value\n         '''\n        self.model_type = \"Transformer\"\n        self.encoder = nn.Embedding(ntoken,nhid) # fill me, nhid = the dim_embed\n        self.pos_encoder = PositionalEncoding(nhid,dropout) #fill me, the PositionalEncoding class is implemented in the next cell\n        encoder_layers = nn.TransformerEncoderLayer(nhid,nhead,nhid,dropout) #fill me we assume nhid = d_model = dim_feedforward\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers,nlayers) #fill me\n        self.nhid = nhid\n        self.init_weights()\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = (\n            mask.float()\n            .masked_fill(mask == 0, float(\"-inf\"))\n            .masked_fill(mask == 1, float(0.0))\n        )\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src, src_mask):\n        src = self.encoder(src) * math.sqrt(self.nhid) \n        src =  self.pos_encoder(src) #fill me\n        output = self.transformer_encoder(src, src_mask)\n        return output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassificationHead(nn.Module):\n    def __init__(self, nhid, nclasses):\n        super(ClassificationHead, self).__init__()\n        self.decoder = nn.Linear(nhid, nclasses)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        output = self.decoder(src)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, ntoken, nhead, nhid, nlayers, nclasses, dropout=0.5):\n        super(Model, self).__init__()\n        self.base =TransformerModel(ntoken, nhead, nhid, nlayers, dropout) #fill me\n        self.classifier = ClassificationHead(nhid, nclasses) #fill me \n\n    def forward(self, src, src_mask):\n        # base model\n        x = self.base.forward(src, src_mask)#fill me\n        # classifier model\n        output =self.classifier.forward(x) #fill me\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, nhid, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, nhid)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, nhid, 2).float() * (-math.log(10000.0) / nhid)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[: x.size(0), :]\n        return self.dropout(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ntokens = 100 #fill me # the size of vocabulary\nnhid = 200  # hidden dimension\nnlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2  # the number of heads in the multiheadattention models\ndropout = 0  # the dropout value\n\nmodel = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)\ndummy_input = torch.tensor([[2, 6, 2, 5, 43, 21]]).to(device)\nsrc_mask = model.base.generate_square_subsequent_mask(1).to(device)\nout = model.forward(dummy_input, src_mask)\n\nprint(out.shape) # is it the right shape?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n!head -5 dict.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_vocab = \"dict.txt\"\ntoken2ind = {\"<sos>\": 0, \"<pad>\": 1, \"<eos>\": 2, \"<oov>\": 3} # the 4 first indices are reserved to special tokens\nc = 4\nwith open(path_vocab, \"r\") as f:\n    for idx, line in enumerate(f):\n        word = line.split()[0].strip()\n        if word not in token2ind.keys() :\n          token2ind[word] =  c #fill me\n          c+=1\n\nind2token = {v: k for k, v in token2ind.items()}  #fill me\n\nprint(ind2token[1111])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass Dataset(Dataset):\n    def __init__(\n        self,\n        path_documents,\n        path_labels=None,\n        token2ind={},\n        max_len=512,\n        task=\"language_modeling\",\n    ):\n        self.task = task\n        self.max_len = max_len\n        self.token2ind = token2ind\n        self.documents = []\n        self.labels = []\n        with open(path_documents, \"r\") as f1:\n            for line in f1:\n                self.documents.append(line.strip())\n        if task == \"classification\":\n            with open(path_labels, \"r\") as f1:\n                for line in f1:\n                    self.labels.append(int(line.strip()))\n            assert len(self.labels) == len(self.documents)\n\n    def __len__(self):\n        return len(self.documents)\n\n    def __getitem__(self, index):\n        sequence = self.documents[index].split()\n        if len(sequence) > self.max_len - 1:\n            sequence = sequence[: self.max_len - 1]\n        source_sequence = [self.token2ind[\"<sos>\"]] + [\n            self.token2ind[word] if word in self.token2ind else self.token2ind[\"<oov>\"]\n            for word in sequence[: self.max_len]\n        ]\n        if self.task == \"language_modeling\":\n            target = source_sequence[1:]\n            target.append(self.token2ind[\"<eos>\"])\n        elif self.task == \"classification\":\n            target = [self.labels[index]]\n        sample = {\n            \"source_sequence\": torch.tensor(source_sequence),\n            \"target\": torch.tensor(target),\n        }\n        return sample\n\n\ndef MyCollator(batch):\n    source_sequences = pad_sequence(\n        #we use padding to match the length of the sequences in the same batch\n        [sample[\"source_sequence\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n    )\n    target = pad_sequence(\n        [sample[\"target\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n    )\n    return source_sequences, target.reshape(-1)\n\n\ndef get_loader(\n    path_documents,\n    path_labels=None,\n    token2ind={},\n    max_len=512,\n    batch_size=32,\n    task=\"language_modeling\",\n):\n    dataset = Dataset(\n        path_documents,\n        path_labels=path_labels,\n        token2ind=token2ind,\n        max_len=512,\n        task=task,\n    )\n    data_loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=MyCollator,\n        pin_memory=True,\n        drop_last=True,\n    )\n    return data_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(\n    path_data_train,\n    path_labels_train=None,\n    path_data_valid=None,\n    save_interval=-1,\n    log_interval=5,\n    task=\"language_modeling\",\n    batch_size=32,\n):\n    model.train()\n    total_loss = 0.0\n    ntokens = len(token2ind)\n    data_loader = get_loader(\n        path_data_train,\n        path_labels_train,\n        token2ind,\n        task=task,\n        batch_size=batch_size,\n    )\n    \n    losses = []\n    for idx, data in enumerate(data_loader): #step 1\n        \n        optimizer.zero_grad()\n        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n            device\n        )\n        input = data[0].to(device)\n        output = model(input, src_mask) #step 2\n        if task == 'classification':\n            #last vector only\n            output = model(src, src_mask) #fill me\n        \n        output = output.view(-1, output.shape[-1])\n        target = data[1]#fill me   ################\n        \n        target = target.to(device)\n\n        loss =  criterion(output, target) #fill me, Cross entropy check next cells\n        #fill me step 3\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n        #fill me step 4\n\n        total_loss += loss.item() \n        if idx % log_interval == 0 and idx > 0:\n            cur_loss = total_loss / log_interval\n            print(\n                \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n                \"loss {:5.5f} | ppl {:8.3f}\".format(\n                    epoch, idx, len(data_loader), cur_loss, math.exp(cur_loss),\n                )\n            )\n            losses.append(cur_loss)\n            total_loss = 0\n    return losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ntokens = len(ind2token) #fill me # the size of vocabulary\nnhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2  # the number of heads in the multiheadattention models\ndropout = 0  # the dropout value\n\nnclasses = 2 # for classification task only\n\nmodel = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimization paramerters\n\ncriterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\nlr = 0.0003  # learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\npath_data_train = \"pretraining_subset.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pretraining on a tiny subset\nlog_interval = 500\nepochs = 2\nfor epoch in range(1, epochs + 1): #5\n    train(\n        path_data_train,\n        save_interval=-1,\n        task=\"language_modeling\",\n        batch_size=16,\n        log_interval=log_interval,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}