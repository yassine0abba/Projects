{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALTEGRAD_2020_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAfI8mCWAf3"
      },
      "source": [
        "# Transfer learning for NLP\n",
        "##### November 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqukuIe0Rb_c"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF6fjkqgN39"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0cj9WkSFQwl"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        '''\n",
        "        ntokens: the size of vocabulary\n",
        "        nhid: the hidden dimension of the model.\n",
        "        We assume that embedding_dim = nhid\n",
        "        nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "        nhead: the number of heads in the multiheadattention models\n",
        "        dropout: the dropout value\n",
        "         '''\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.encoder = nn.Embedding(ntoken,nhid) # fill me, nhid = the dim_embed\n",
        "        self.pos_encoder = PositionalEncoding(nhid,dropout) #fill me, the PositionalEncoding class is implemented in the next cell\n",
        "        encoder_layers = nn.TransformerEncoderLayer(nhid,nhead,nhid,dropout) #fill me we assume nhid = d_model = dim_feedforward\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers,nlayers) #fill me\n",
        "        self.nhid = nhid\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.nhid) \n",
        "        src =  self.pos_encoder(src) #fill me\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        return output\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oa9JyRW8CdO"
      },
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, nhid, nclasses):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.decoder = nn.Linear(nhid, nclasses)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = self.decoder(src)\n",
        "        return output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpqyX6YW8HSf"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, nclasses, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.base =TransformerModel(ntoken, nhead, nhid, nlayers, dropout) #fill me\n",
        "        self.classifier = ClassificationHead(nhid, nclasses) #fill me \n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # base model\n",
        "        x = self.base.forward(src, src_mask)#fill me\n",
        "        # classifier model\n",
        "        output =self.classifier.forward(x) #fill me\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt2QQohaFZry"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, nhid, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, nhid)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, nhid, 2).float() * (-math.log(10000.0) / nhid)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfEYHJx2JW6l"
      },
      "source": [
        "Let's verify if our model works, by applying one inference step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhb2gkUhJMR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6e9886-a5e0-4178-f9cd-179ac501e9ea"
      },
      "source": [
        "ntokens = 100 #fill me # the size of vocabulary\n",
        "nhid = 200  # hidden dimension\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)\n",
        "dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21]]).to(device)\n",
        "src_mask = model.base.generate_square_subsequent_mask(1).to(device)\n",
        "out = model.forward(dummy_input, src_mask)\n",
        "\n",
        "print(out.shape) # is it the right shape?"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 6, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74NN897Fcit"
      },
      "source": [
        "## Vocabulary and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qjd26ghWuff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2cc52a7-d56a-4c32-9aea-3f9a98dad6fa"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
        "!head -5 dict.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-05 10:00:35--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577587 (564K) [text/plain]\n",
            "Saving to: ‘dict.txt.2’\n",
            "\n",
            "\rdict.txt.2            0%[                    ]       0  --.-KB/s               \rdict.txt.2          100%[===================>] 564.05K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-12-05 10:00:35 (15.8 MB/s) - ‘dict.txt.2’ saved [577587/577587]\n",
            "\n",
            "▁d 1\n",
            "es 1\n",
            "▁l 1\n",
            "en 1\n",
            "on 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFdH_-JeFbGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b7fa9d-c676-402c-e492-65ab356ce5eb"
      },
      "source": [
        "path_vocab = \"dict.txt\"\n",
        "token2ind = {\"<sos>\": 0, \"<pad>\": 1, \"<eos>\": 2, \"<oov>\": 3} # the 4 first indices are reserved to special tokens\n",
        "c = 4\n",
        "with open(path_vocab, \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        word = line.split()[0].strip()\n",
        "        if word not in token2ind.keys() :\n",
        "          token2ind[word] =  c #fill me\n",
        "          c+=1\n",
        "\n",
        "ind2token = {v: k for k, v in token2ind.items()}  #fill me\n",
        "\n",
        "print(ind2token[1111])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁trop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOExGODajN8p"
      },
      "source": [
        "### Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0jN-Ar9i5Q1"
      },
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_documents,\n",
        "        path_labels=None,\n",
        "        token2ind={},\n",
        "        max_len=512,\n",
        "        task=\"language_modeling\",\n",
        "    ):\n",
        "        self.task = task\n",
        "        self.max_len = max_len\n",
        "        self.token2ind = token2ind\n",
        "        self.documents = []\n",
        "        self.labels = []\n",
        "        with open(path_documents, \"r\") as f1:\n",
        "            for line in f1:\n",
        "                self.documents.append(line.strip())\n",
        "        if task == \"classification\":\n",
        "            with open(path_labels, \"r\") as f1:\n",
        "                for line in f1:\n",
        "                    self.labels.append(int(line.strip()))\n",
        "            assert len(self.labels) == len(self.documents)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = self.documents[index].split()\n",
        "        if len(sequence) > self.max_len - 1:\n",
        "            sequence = sequence[: self.max_len - 1]\n",
        "        source_sequence = [self.token2ind[\"<sos>\"]] + [\n",
        "            self.token2ind[word] if word in self.token2ind else self.token2ind[\"<oov>\"]\n",
        "            for word in sequence[: self.max_len]\n",
        "        ]\n",
        "        if self.task == \"language_modeling\":\n",
        "            target = source_sequence[1:]\n",
        "            target.append(self.token2ind[\"<eos>\"])\n",
        "        elif self.task == \"classification\":\n",
        "            target = [self.labels[index]]\n",
        "        sample = {\n",
        "            \"source_sequence\": torch.tensor(source_sequence),\n",
        "            \"target\": torch.tensor(target),\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def MyCollator(batch):\n",
        "    source_sequences = pad_sequence(\n",
        "        #we use padding to match the length of the sequences in the same batch\n",
        "        [sample[\"source_sequence\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    target = pad_sequence(\n",
        "        [sample[\"target\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    return source_sequences, target.reshape(-1)\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    path_documents,\n",
        "    path_labels=None,\n",
        "    token2ind={},\n",
        "    max_len=512,\n",
        "    batch_size=32,\n",
        "    task=\"language_modeling\",\n",
        "):\n",
        "    dataset = Dataset(\n",
        "        path_documents,\n",
        "        path_labels=path_labels,\n",
        "        token2ind=token2ind,\n",
        "        max_len=512,\n",
        "        task=task,\n",
        "    )\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=MyCollator,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return data_loader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTns4lHrjUTa"
      },
      "source": [
        "## The Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_jwosiLjRsS"
      },
      "source": [
        "def train(\n",
        "    path_data_train,\n",
        "    path_labels_train=None,\n",
        "    path_data_valid=None,\n",
        "    save_interval=-1,\n",
        "    log_interval=5,\n",
        "    task=\"language_modeling\",\n",
        "    batch_size=32,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    ntokens = len(token2ind)\n",
        "    data_loader = get_loader(\n",
        "        path_data_train,\n",
        "        path_labels_train,\n",
        "        token2ind,\n",
        "        task=task,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    \n",
        "    losses = []\n",
        "    for idx, data in enumerate(data_loader): #step 1\n",
        "        optimizer.zero_grad()\n",
        "        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "        input = data[0].to(device)\n",
        "        output = model(input, src_mask) #step 2\n",
        "        if task == 'classification':\n",
        "            #last vector only\n",
        "            output = torch.reshape(output[-1,:,:],(1,output.size(1),output.size(2))) #fill me\n",
        "        \n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target = data[1]#fill me   ################\n",
        "        \n",
        "        target = target.to(device)\n",
        "\n",
        "        loss =  criterion(output, target) #fill me, Cross entropy check next cells\n",
        "        #fill me step 3\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
        "        #fill me step 4\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() \n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
        "                \"loss {:5.5f} | ppl {:8.3f}\".format(\n",
        "                    epoch, idx, len(data_loader), cur_loss, math.exp(cur_loss),\n",
        "                )\n",
        "            )\n",
        "            losses.append(cur_loss)\n",
        "            total_loss = 0\n",
        "    return losses"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgf6BDB9jUr6"
      },
      "source": [
        "ntokens = len(ind2token) #fill me # the size of vocabulary\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "nclasses = 2 # for classification task only\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-OLy4KIkDwf"
      },
      "source": [
        "# optimization paramerters\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
        "lr = 0.0003  # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwh3n9xZQy4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79506ca-36bf-4e79-aad5-40a82aece001"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
        "path_data_train = \"pretraining_subset.txt\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-05 10:00:36--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10146460 (9.7M) [text/plain]\n",
            "Saving to: ‘pretraining_subset.txt.2’\n",
            "\n",
            "\rpretraining_subset.   0%[                    ]       0  --.-KB/s               \rpretraining_subset. 100%[===================>]   9.68M  57.2MB/s    in 0.2s    \n",
            "\n",
            "2020-12-05 10:00:36 (57.2 MB/s) - ‘pretraining_subset.txt.2’ saved [10146460/10146460]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m11g4ScjZaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d468a7c4-0cb3-4447-8100-5165f79b0013"
      },
      "source": [
        "#pretraining on a tiny subset\n",
        "log_interval = 500\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1): #5\n",
        "    train(\n",
        "        path_data_train,\n",
        "        save_interval=-1,\n",
        "        task=\"language_modeling\",\n",
        "        batch_size=16,\n",
        "        log_interval=log_interval,\n",
        "    )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 3125 steps | loss 7.31725 | ppl 1506.053\n",
            "| epoch   1 |  1000/ 3125 steps | loss 6.49436 | ppl  661.404\n",
            "| epoch   1 |  1500/ 3125 steps | loss 6.19609 | ppl  490.828\n",
            "| epoch   1 |  2000/ 3125 steps | loss 6.02450 | ppl  413.436\n",
            "| epoch   1 |  2500/ 3125 steps | loss 5.93682 | ppl  378.729\n",
            "| epoch   1 |  3000/ 3125 steps | loss 5.82728 | ppl  339.433\n",
            "| epoch   2 |   500/ 3125 steps | loss 5.53631 | ppl  253.741\n",
            "| epoch   2 |  1000/ 3125 steps | loss 5.48440 | ppl  240.905\n",
            "| epoch   2 |  1500/ 3125 steps | loss 5.45337 | ppl  233.543\n",
            "| epoch   2 |  2000/ 3125 steps | loss 5.41625 | ppl  225.033\n",
            "| epoch   2 |  2500/ 3125 steps | loss 5.38461 | ppl  218.026\n",
            "| epoch   2 |  3000/ 3125 steps | loss 5.37656 | ppl  216.278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM1dOvkO4e"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BcBC6FSkMH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a666a458-7b8f-4950-c416-618a0c35c2d2"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device)\n",
        "\n",
        "#load the checkpoint\n",
        "checkpoint = torch.load('pretrained_model_4layers.pt') \n",
        "#load state dict\n",
        "model.load_state_dict(checkpoint['model_state_dict']) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-05 10:06:18--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88093955 (84M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_model_4layers.pt’\n",
            "\n",
            "pretrained_model_4l 100%[===================>]  84.01M   186MB/s    in 0.5s    \n",
            "\n",
            "2020-12-05 10:06:21 (186 MB/s) - ‘pretrained_model_4layers.pt’ saved [88093955/88093955]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBRRVsWqlIoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c87b94-418f-47e8-e701-bf97701c26dd"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
        "\n",
        "#examples\n",
        "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
        "decoded = s.decode_pieces(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "--2020-12-05 10:06:25--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115362 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘sentencepiece.french.model’\n",
            "\n",
            "sentencepiece.frenc 100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-12-05 10:06:25 (25.6 MB/s) - ‘sentencepiece.french.model’ saved [1115362/1115362]\n",
            "\n",
            "['▁Bonjour', '▁les', '▁amis', '!']\n",
            "Bonjour les amis!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtLlV05pkQI3"
      },
      "source": [
        "import numpy as np\n",
        "def infer_next_token(sent):\n",
        "    model.eval()\n",
        "    sent_pieces = s.encode_as_pieces(sent)\n",
        "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces]\n",
        "    source = torch.tensor(source).to(device)\n",
        "    source = source.reshape(-1, 1)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
        "    out = model(source, src_mask)\n",
        "    next_token_ind = out[-1].argmax(-1)\n",
        "    return next_token_ind, out\n",
        "    \n",
        "def infer_next_tokens(sent, max_len=50):\n",
        "    # to be implemented\n",
        "    sent_encoded = s.encode_as_pieces(sent)\n",
        "    count = 0 \n",
        "    while count < max_len :\n",
        "      new_sent = s.decode_pieces(sent_encoded)\n",
        "      z = np.array(infer_next_token(new_sent)[0].cpu())\n",
        "      x = ind2token[z[0]]\n",
        "      count+=1\n",
        "      sent_encoded.append(x)\n",
        "      if x == '<eos>' : \n",
        "        return s.decode_pieces(sent_encoded)\n",
        "    return s.decode_pieces(sent_encoded)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f83Nn5nSly4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "558ff49b-1333-43d0-8458-3b38bd2b2773"
      },
      "source": [
        "sent = \"Bonjour les\"\n",
        "infer_next_tokens(sent)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bonjour les gens qui ont été très accueillants et sympathiques.<eos>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7mjVzomoZ3"
      },
      "source": [
        "### Supervised task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K1BZsblmEmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dcae276-4150-4da8-9784-1f674438210a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
        "\n",
        "path_data_train = \"train.review.spm\"\n",
        "path_labels_train = \"train.label\"\n",
        "\n",
        "path_data_valid = \"test.review.spm\"\n",
        "path_labels_valid = \"test.label\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-05 10:06:25--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1495960 (1.4M) [text/plain]\n",
            "Saving to: ‘train.review.spm’\n",
            "\n",
            "train.review.spm    100%[===================>]   1.43M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-12-05 10:06:26 (27.5 MB/s) - ‘train.review.spm’ saved [1495960/1495960]\n",
            "\n",
            "--2020-12-05 10:06:26--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200 (3.1K) [text/plain]\n",
            "Saving to: ‘train.label’\n",
            "\n",
            "train.label         100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-05 10:06:26 (54.3 MB/s) - ‘train.label’ saved [3200/3200]\n",
            "\n",
            "--2020-12-05 10:06:26--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864544 (1.8M) [text/plain]\n",
            "Saving to: ‘test.review.spm’\n",
            "\n",
            "test.review.spm     100%[===================>]   1.78M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-12-05 10:06:26 (43.4 MB/s) - ‘test.review.spm’ saved [1864544/1864544]\n",
            "\n",
            "--2020-12-05 10:06:27--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4000 (3.9K) [text/plain]\n",
            "Saving to: ‘test.label’\n",
            "\n",
            "test.label          100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-05 10:06:27 (65.9 MB/s) - ‘test.label’ saved [4000/4000]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MLfvjiom2SL"
      },
      "source": [
        "# a function to evaluate the validation accuracy of the model.\n",
        "def evaluate_accuracy(data_loader):\n",
        "  batch_size = data_loader.batch_size\n",
        "  accs = []\n",
        "  for idx, data in enumerate(data_loader):\n",
        "    input = data[0].to(device)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to( device)\n",
        "    output = model(input,src_mask)\n",
        "    soft = nn.Softmax( dim = 1)\n",
        "    predicted_labels=soft(output[-1,:,:]).argmax(-1)\n",
        "    true_labels = data[1]\n",
        "    true_labels = true_labels.to(device)\n",
        "    acc = ((predicted_labels == true_labels).float().sum()).cpu().numpy()/batch_size\n",
        "    accs.append(acc)\n",
        "  return np.mean(accs) \n",
        "    #to be implemented"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzmx7T7xoa6v"
      },
      "source": [
        "#save the base model to be loaded later in the fine-tuning phase\n",
        "torch.save({\"model_state_dict\": model.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-xclMCpnVpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc50359-170d-45aa-894e-32917419e51e"
      },
      "source": [
        "from_scratch_settings = [True, False]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            path_data_train,\n",
        "            path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task='classification',\n",
        "            batch_size=8,\n",
        "            log_interval=50,\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            get_loader(\n",
        "                path_data_valid,\n",
        "                path_labels_valid,\n",
        "                token2ind=token2ind,\n",
        "                batch_size=20,\n",
        "                task='classification',\n",
        "            )\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "    print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/  200 steps | loss 0.73566 | ppl    2.087\n",
            "| epoch   1 |   100/  200 steps | loss 0.75581 | ppl    2.129\n",
            "| epoch   1 |   150/  200 steps | loss 0.69754 | ppl    2.009\n",
            "| epoch   2 |    50/  200 steps | loss 0.64753 | ppl    1.911\n",
            "| epoch   2 |   100/  200 steps | loss 0.59064 | ppl    1.805\n",
            "| epoch   2 |   150/  200 steps | loss 0.58983 | ppl    1.804\n",
            "| epoch   3 |    50/  200 steps | loss 0.44666 | ppl    1.563\n",
            "| epoch   3 |   100/  200 steps | loss 0.36190 | ppl    1.436\n",
            "| epoch   3 |   150/  200 steps | loss 0.36651 | ppl    1.443\n",
            "| epoch   4 |    50/  200 steps | loss 0.23432 | ppl    1.264\n",
            "| epoch   4 |   100/  200 steps | loss 0.13246 | ppl    1.142\n",
            "| epoch   4 |   150/  200 steps | loss 0.12470 | ppl    1.133\n",
            "| epoch   5 |    50/  200 steps | loss 0.02007 | ppl    1.020\n",
            "| epoch   5 |   100/  200 steps | loss 0.03804 | ppl    1.039\n",
            "| epoch   5 |   150/  200 steps | loss 0.07842 | ppl    1.082\n",
            "| epoch   6 |    50/  200 steps | loss 0.04450 | ppl    1.046\n",
            "| epoch   6 |   100/  200 steps | loss 0.00878 | ppl    1.009\n",
            "| epoch   6 |   150/  200 steps | loss 0.00461 | ppl    1.005\n",
            "| epoch   7 |    50/  200 steps | loss 0.00878 | ppl    1.009\n",
            "| epoch   7 |   100/  200 steps | loss 0.02413 | ppl    1.024\n",
            "| epoch   7 |   150/  200 steps | loss 0.00011 | ppl    1.000\n",
            "| epoch   8 |    50/  200 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   8 |   100/  200 steps | loss 0.01931 | ppl    1.020\n",
            "| epoch   8 |   150/  200 steps | loss 0.00105 | ppl    1.001\n",
            "| epoch   9 |    50/  200 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   9 |   100/  200 steps | loss 0.00085 | ppl    1.001\n",
            "| epoch   9 |   150/  200 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch  10 |    50/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  10 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  10 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  15 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  15 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "\n",
            "=====PRETRAINED MODEL======\n",
            "| epoch   1 |    50/  200 steps | loss 0.75149 | ppl    2.120\n",
            "| epoch   1 |   100/  200 steps | loss 0.63241 | ppl    1.882\n",
            "| epoch   1 |   150/  200 steps | loss 0.63346 | ppl    1.884\n",
            "| epoch   2 |    50/  200 steps | loss 0.44334 | ppl    1.558\n",
            "| epoch   2 |   100/  200 steps | loss 0.45368 | ppl    1.574\n",
            "| epoch   2 |   150/  200 steps | loss 0.53917 | ppl    1.715\n",
            "| epoch   3 |    50/  200 steps | loss 0.36599 | ppl    1.442\n",
            "| epoch   3 |   100/  200 steps | loss 0.41462 | ppl    1.514\n",
            "| epoch   3 |   150/  200 steps | loss 0.36419 | ppl    1.439\n",
            "| epoch   4 |    50/  200 steps | loss 0.28604 | ppl    1.331\n",
            "| epoch   4 |   100/  200 steps | loss 0.24916 | ppl    1.283\n",
            "| epoch   4 |   150/  200 steps | loss 0.29271 | ppl    1.340\n",
            "| epoch   5 |    50/  200 steps | loss 0.21103 | ppl    1.235\n",
            "| epoch   5 |   100/  200 steps | loss 0.16980 | ppl    1.185\n",
            "| epoch   5 |   150/  200 steps | loss 0.23957 | ppl    1.271\n",
            "| epoch   6 |    50/  200 steps | loss 0.09603 | ppl    1.101\n",
            "| epoch   6 |   100/  200 steps | loss 0.08890 | ppl    1.093\n",
            "| epoch   6 |   150/  200 steps | loss 0.10994 | ppl    1.116\n",
            "| epoch   7 |    50/  200 steps | loss 0.04864 | ppl    1.050\n",
            "| epoch   7 |   100/  200 steps | loss 0.04478 | ppl    1.046\n",
            "| epoch   7 |   150/  200 steps | loss 0.04757 | ppl    1.049\n",
            "| epoch   8 |    50/  200 steps | loss 0.00045 | ppl    1.000\n",
            "| epoch   8 |   100/  200 steps | loss 0.01564 | ppl    1.016\n",
            "| epoch   8 |   150/  200 steps | loss 0.01141 | ppl    1.011\n",
            "| epoch   9 |    50/  200 steps | loss 0.00014 | ppl    1.000\n",
            "| epoch   9 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   150/  200 steps | loss 0.00549 | ppl    1.006\n",
            "| epoch  10 |    50/  200 steps | loss 0.00486 | ppl    1.005\n",
            "| epoch  10 |   100/  200 steps | loss 0.00010 | ppl    1.000\n",
            "| epoch  10 |   150/  200 steps | loss 0.00118 | ppl    1.001\n",
            "| epoch  11 |    50/  200 steps | loss 0.00642 | ppl    1.006\n",
            "| epoch  11 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  11 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |    50/  200 steps | loss 0.02189 | ppl    1.022\n",
            "| epoch  12 |   100/  200 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch  12 |   150/  200 steps | loss 0.01085 | ppl    1.011\n",
            "| epoch  13 |    50/  200 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.01034 | ppl    1.010\n",
            "| epoch  13 |   150/  200 steps | loss 0.00320 | ppl    1.003\n",
            "| epoch  14 |    50/  200 steps | loss 0.02551 | ppl    1.026\n",
            "| epoch  14 |   100/  200 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  14 |   150/  200 steps | loss 0.00298 | ppl    1.003\n",
            "| epoch  15 |    50/  200 steps | loss 0.02417 | ppl    1.024\n",
            "| epoch  15 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  15 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCpBIdTHojm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "7cc4f36b-57a4-4f78-c031-cff43204ac82"
      },
      "source": [
        "#Visualize the accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(len(pretrained_valid_acc)),pretrained_valid_acc, label = \"pretrained\")\n",
        "plt.plot(np.arange(len(from_scratch_valid_acc)),from_scratch_valid_acc, label = \"from scratch\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuraccy \")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73UkXtz5yf0l"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}
